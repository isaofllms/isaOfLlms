{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts0ASBtreCsv"
      },
      "source": [
        "# Setting Up"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Install all needed packages in one go\n",
        "!pip install mistralai cohere openai python-dotenv colorama openpyxl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRflG3v5FHLG",
        "outputId": "779d8acc-ce45-4391-a7a7-4dfa4ce5af86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mistralai\n",
            "  Downloading mistralai-1.7.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting cohere\n",
            "  Downloading cohere-5.15.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.1)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.4.0)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.33.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.21.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20250515-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (4.13.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere) (0.31.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Downloading mistralai-1.7.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.5/301.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.15.0-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading fastavro-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20250515-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: types-requests, python-dotenv, httpx-sse, fastavro, eval-type-backport, colorama, mistralai, cohere\n",
            "Successfully installed cohere-5.15.0 colorama-0.4.6 eval-type-backport-0.2.2 fastavro-1.11.1 httpx-sse-0.4.0 mistralai-1.7.0 python-dotenv-1.1.0 types-requests-2.32.0.20250515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Standard-library imports\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "\n",
        "# 3) Third-party imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
        "\n",
        "import cohere\n",
        "from mistralai import Mistral\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from colorama import Fore\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "BE_rDh5Y8IuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmpj8cM8OBoQ"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K27reATwdpTF"
      },
      "source": [
        "### ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OpenAIclient = OpenAI(\n",
        "  api_key=userdata.get(\"OPENAI_API_KEY\")\n",
        "  )\n",
        "\n",
        "def get_gpt_answer(scenario,temp):\n",
        "  completion = OpenAIclient.chat.completions.create(\n",
        "      model=\"gpt-4.1-mini\",\n",
        "      messages=[\n",
        "             {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": scenario\n",
        "             }\n",
        "\n",
        "        ],\n",
        "      temperature=temp,\n",
        "      max_tokens=3000\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "mYdMUfgB-ivQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqPBiFNte-MQ"
      },
      "source": [
        "### Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M__YZu_rfAbt"
      },
      "outputs": [],
      "source": [
        "# # load your key however you like\n",
        "# GOOGLE_API_KEY = userdata.get(\"GOOGLE_GENAI_API_KEY\")\n",
        "# genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# def get_gemini_answer(scenario: str, temp: float) -> str:\n",
        "#     # build a new model instance with the right sampling settings\n",
        "#     temp_model = genai.GenerativeModel(\n",
        "#         \"gemini-2.0-flash\",\n",
        "#         generation_config=genai.GenerationConfig(\n",
        "#             max_output_tokens=3000,\n",
        "#             temperature=temp,\n",
        "#         ),\n",
        "#     )\n",
        "#     # now generate, passing only the content\n",
        "#     response = temp_model.generate_content(scenario)\n",
        "#     time.sleep(7)\n",
        "#     return response.text\n",
        "\n",
        "GeminiClient = OpenAI(\n",
        "    api_key=userdata.get(\"DEEPINFRA_API_KEY\"),\n",
        "    base_url=\"https://api.deepinfra.com/v1/openai\"\n",
        ")\n",
        "\n",
        "def get_gemini_answer(scenario: str, temp: float) -> str:\n",
        "  chat_completion = GeminiClient.chat.completions.create(\n",
        "      model=\"google/gemini-2.0-flash-001\",\n",
        "      messages=[\n",
        "             {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": scenario\n",
        "             }\n",
        "\n",
        "        ],\n",
        "      max_tokens=2000,\n",
        "      temperature=temp\n",
        "  )\n",
        "\n",
        "  return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxCVXAu1gBuF"
      },
      "source": [
        "### llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1ftRxU3gDO8"
      },
      "outputs": [],
      "source": [
        "LlamaClient = OpenAI(\n",
        "  api_key=userdata.get(\"DEEPINFRA_API_KEY\"),\n",
        "  base_url=\"https://api.deepinfra.com/v1/openai\",\n",
        "  )\n",
        "\n",
        "\n",
        "def get_llama_answer(scenario,temp):\n",
        "  completion = LlamaClient.chat.completions.create(\n",
        "      model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
        "      messages=[\n",
        "             {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": scenario\n",
        "             }\n",
        "\n",
        "        ],\n",
        "      temperature=temp,\n",
        "      max_tokens=3000\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlvJz8M-9OSN"
      },
      "source": [
        "### Mixtral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIKd1hZI-EQ9"
      },
      "outputs": [],
      "source": [
        "MixtralClient = OpenAI(\n",
        "  api_key = userdata.get(\"DEEPINFRA_API_KEY\"),\n",
        "  base_url=\"https://api.deepinfra.com/v1/openai\",\n",
        "  )\n",
        "\n",
        "\n",
        "def get_mixtral_answer(scenario,temp):\n",
        "    chat_completion = MixtralClient.chat.completions.create(\n",
        "        model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "        messages=[\n",
        "            {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": scenario\n",
        "              }\n",
        "            ],\n",
        "        max_tokens=3000,\n",
        "        temperature=temp\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7PJn05v_HR2"
      },
      "source": [
        "### Phi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2e8anO__YYh"
      },
      "outputs": [],
      "source": [
        "PhiClient = OpenAI(\n",
        "    api_key=userdata.get(\"DEEPINFRA_API_KEY\"),\n",
        "    base_url=\"https://api.deepinfra.com/v1/openai\"\n",
        ")\n",
        "\n",
        "\n",
        "def get_phi_answer(scenario,temp):\n",
        "  completion = PhiClient.chat.completions.create(\n",
        "      model=\"microsoft/phi-4\",\n",
        "      messages=[\n",
        "             {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": scenario\n",
        "             }\n",
        "\n",
        "        ],\n",
        "      temperature=temp,\n",
        "      max_tokens=3000,\n",
        "      stream=False\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFTVh9TMJW5T"
      },
      "source": [
        "### Gemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezCBZBJ_J_CX"
      },
      "outputs": [],
      "source": [
        "GemmaClient = OpenAI(\n",
        "  api_key=userdata.get(\"DEEPINFRA_API_KEY\"),\n",
        "  base_url=\"https://api.deepinfra.com/v1/openai\",\n",
        "  )\n",
        "\n",
        "def get_gemma_answer(scenario,temp):\n",
        "  completion = GemmaClient.chat.completions.create(\n",
        "      model=\"google/gemma-3-27b-it\",\n",
        "      messages=[\n",
        "             {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": scenario\n",
        "             }\n",
        "\n",
        "        ],\n",
        "      temperature=temp,\n",
        "      max_tokens=3000\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM9yPVgnLGK7"
      },
      "source": [
        "### Cohere Command A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QUp1B50Lm7H"
      },
      "outputs": [],
      "source": [
        "co = cohere.Client(api_key = userdata.get('COHERE_API_KEY'))\n",
        "\n",
        "def get_cohere_answer(scenario,temp):\n",
        "\n",
        "  response = co.chat(\n",
        "    model=\"command-a-03-2025\",\n",
        "    message=scenario,\n",
        "    temperature=temp,\n",
        "    max_tokens=3000\n",
        "  )\n",
        "  time.sleep(6.1)\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Claude"
      ],
      "metadata": {
        "id": "4SR_K1ReAFIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ClaideClient = OpenAI(\n",
        "  api_key=userdata.get(\"DEEPINFRA_API_KEY\"),\n",
        "  base_url=\"https://api.deepinfra.com/v1/openai\",\n",
        "  )\n",
        "\n",
        "def get_claude_answer(scenario,temp):\n",
        "  completion = ClaideClient.chat.completions.create(\n",
        "      model=\"anthropic/claude-3-7-sonnet-latest\",\n",
        "      messages=[\n",
        "             {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": scenario\n",
        "             }\n",
        "\n",
        "        ],\n",
        "      temperature=temp,\n",
        "      max_tokens=3000\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "CQqtOU6MAG90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepSeek"
      ],
      "metadata": {
        "id": "fVG8abftAPuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DeepSeekClient = OpenAI(\n",
        "  api_key=userdata.get(\"DEEPINFRA_API_KEY\"),\n",
        "  base_url=\"https://api.deepinfra.com/v1/openai\",\n",
        "  )\n",
        "\n",
        "def get_deepseek_answer(scenario,temp):\n",
        "  completion = DeepSeekClient.chat.completions.create(\n",
        "      model=\"deepseek-ai/DeepSeek-V3-0324\",\n",
        "      messages=[\n",
        "             {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": scenario\n",
        "             }\n",
        "\n",
        "        ],\n",
        "      temperature=temp,\n",
        "      max_tokens=1000\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "NKuSmLRMAQrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mistral"
      ],
      "metadata": {
        "id": "s2J1xSBBAoYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MistralClient = OpenAI(\n",
        "  api_key=userdata.get(\"DEEPINFRA_API_KEY\"),\n",
        "  base_url=\"https://api.deepinfra.com/v1/openai\",\n",
        "  )\n",
        "\n",
        "def get_mistral_answer(scenario, temp):\n",
        "  completion = MistralClient.chat.completions.create(\n",
        "      model=\"mistralai/Mistral-Small-24B-Instruct-2501\",\n",
        "      messages=[\n",
        "             {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": scenario\n",
        "             }\n",
        "\n",
        "        ],\n",
        "      temperature=temp,\n",
        "      max_tokens=3000\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "G_1CP0AQAp2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUzMFb8f8LKU"
      },
      "source": [
        "# Create Temps Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gQsylhHalgG"
      },
      "outputs": [],
      "source": [
        "models = [\n",
        "        'GPT-4.1-mini',\n",
        "          'Gemini-2.0-flash',\n",
        "          'Llama-4-Scout',\n",
        "          'Mixtral-8x7B',\n",
        "          'Phi-4',\n",
        "          'Gemma-3',\n",
        "          'Command-A',\n",
        "          'Claude-3.7-Sonnet',\n",
        "          'DeepSeek-V3',\n",
        "          'Mistral-Small-3',\n",
        "          ]\n",
        "get_answer = {\n",
        "    'GPT-4.1-mini'                    : get_gpt_answer,\n",
        "    'Gemini-2.0-flash'                : get_gemini_answer,\n",
        "    'Llama-4-Scout'  : get_llama_answer,\n",
        "    'Mixtral-8x7B'      : get_mixtral_answer,\n",
        "    'Phi-4'                           : get_phi_answer,\n",
        "    'Gemma-3'                  : get_gemma_answer,\n",
        "    'Command-A'               : get_cohere_answer,\n",
        "    'Claude-3-7-Sonnet'        : get_claude_answer,\n",
        "    'DeepSeek-V3'                : get_deepseek_answer,\n",
        "    'Mistral-Small-3' : get_mistral_answer,\n",
        "}\n",
        "\n",
        "temps_lst=[0.25, 0.5, 0.75, 1.0]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/Criterinos and Scenarios.xlsx')\n",
        "\n",
        "for run_idx in range(1, 11):\n",
        "    # build a fresh results DF for this run\n",
        "    results = pd.DataFrame(index=df.index)\n",
        "\n",
        "    for model_name in models:\n",
        "        func = get_answer[model_name]\n",
        "        for temp in temps_lst:\n",
        "            print(f\"This is run number {1}, on model {model_name}, in temperature={temp}\")\n",
        "            col_name = f\"{model_name} Answer in temp={temp}\"\n",
        "            results[col_name] = df[\"Scenario\"].apply(\n",
        "                lambda sc, f=func, t=temp: f(sc, t)\n",
        "            )\n",
        "\n",
        "    # save to a run-specific file\n",
        "    out_name = f\"all_model_answers_run{run_idx}.csv\"\n",
        "    results.to_csv(out_name, index=False)\n",
        "    print(f\"Saved run {run_idx} → {out_name}\")"
      ],
      "metadata": {
        "id": "Rex0rr5UDNWC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "K27reATwdpTF",
        "eqPBiFNte-MQ",
        "-VmNSDW_48SF",
        "BlvJz8M-9OSN",
        "k7PJn05v_HR2",
        "rFTVh9TMJW5T",
        "qM9yPVgnLGK7",
        "FgWUHVGyPnaF",
        "z02gQlh1VMTZ",
        "0oE1izyDzzJd"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}